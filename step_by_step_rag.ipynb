{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3ab816",
   "metadata": {},
   "source": [
    "# Création d'un RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf00ecb7",
   "metadata": {},
   "source": [
    "## Récolte des données et préparations des chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "816aa336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "ai_act_url = \"https://eur-lex.europa.eu/legal-content/FR/TXT/HTML/?uri=OJ:L_202401689\"\n",
    "soup = BeautifulSoup(requests.get(ai_act_url).content, \"html.parser\")\n",
    "text = []\n",
    "context = []\n",
    "url = []\n",
    "url_sections = []\n",
    "url_chapitres = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d81b88",
   "metadata": {},
   "source": [
    "&emsp;La préparation des données et métadonné ce fait ici à l'aide de **BeautifulSoup** et non pas les [**document loaders de LangChain**]() car il nous faut plus de control sur la façon dont on séctionne l'AI Act. \n",
    "\n",
    "&emsp; On cherche à le séctionner en fonction de ses *Considerants*, *Chapitres*, *Sections* et *Articles*.\n",
    "\n",
    "Chacun de ces éléments seront accompagner de métadonnées tel qu'un liens à la séction, le nom du chapitre et de l'article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ec26005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preambule Parser\n",
    "considerants = soup.find_all(\"div\", {\"class\":\"eli-subdivision\", \"id\": re.compile(r'rct_\\d+')})\n",
    "text += [considerant.text.strip() for considerant in considerants]\n",
    "context += [f\"Considérant {n+1}\" for n in range(len(considerants))]\n",
    "url += [ai_act_url+f\"#rct_{n+1}\" for n in range(len(considerants))]\n",
    "url_sections += [None for _ in range(len(considerants))]\n",
    "url_chapitres += [ai_act_url+\"#pbl_1\" for _ in range(len(considerants))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6622c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chapitre Parser\n",
    "chapitres = soup.find(\"div\", {\"id\": \"enc_1\"}).find_all(\"div\", {\"id\": re.compile(r'cpt_[XVILC]+')}, recursive=False)\n",
    "articles = []\n",
    "\n",
    "def get_article_and_article_str(soup_element: BeautifulSoup):\n",
    "    _articles = soup_element.find_all(\"div\", {\"class\":\"eli-subdivision\", \"id\": re.compile(r'art_\\d+')})\n",
    "    article_strs = [\n",
    "        f\"Art.{article['id'].strip('art_')}-\"+article.find(\"div\", {\"class\": \"eli-title\"}).text for\n",
    "        article in _articles\n",
    "    ]\n",
    "    return _articles, article_strs\n",
    "\n",
    "for idx_ch, chapitre in enumerate(chapitres):\n",
    "    chapitre_url = ai_act_url+f\"#{chapitre['id']}\"\n",
    "    chapitre_str = f\"Ch.{idx_ch+1}-\"+chapitre.find(\"p\", {\"class\": \"oj-ti-section-2\"}).text.strip(\"\\n\")+\" > \"\n",
    "    chapitre_articles =  chapitre.find_all(\"div\", {\"class\":\"eli-subdivision\", \"id\": re.compile(r'art_\\d+')})\n",
    "    url_chapitres += [chapitre_url for _ in range(len(chapitre_articles))]\n",
    "    text += [article.text.strip() for article in chapitre_articles]\n",
    "    \n",
    "    section_str = \"\"\n",
    "    sections = chapitre.find_all(\"div\", {\"id\":re.compile(r'cpt_[XVILC]+.sct_\\d+')}, recursive=False)\n",
    "    \n",
    "    if sections:\n",
    "        for idx_sct, section in enumerate(sections):\n",
    "            _articles, article_strs = get_article_and_article_str(section)\n",
    "            articles += _articles\n",
    "            url_sections += [ai_act_url+f\"#{section['id']}\" for _ in range(len(_articles))]\n",
    "            \n",
    "            section_str = f\"Sct.{idx_sct+1}-\"+section.find(\"p\", {\"class\": \"oj-ti-section-2\"}).text.strip(\"\\n\")+\" > \"\n",
    "            context += [chapitre_str+section_str+article_str for article_str in article_strs]\n",
    "    else:\n",
    "        _articles, article_strs = get_article_and_article_str(chapitre)\n",
    "        articles += _articles\n",
    "        context += [chapitre_str+article_str for article_str in article_strs]\n",
    "        url_sections += [None for _ in range(len(_articles))]\n",
    "    \n",
    "url += [ai_act_url+f\"#art_{n}\" for n in range(1,len(articles)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce129ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "annexes = soup.find_all(\"div\", {\"class\": \"eli-container\", \"id\": re.compile(r'anx_[XVILC]+')})\n",
    "titres_annexes = [annexe.find(\"p\", {\"class\": \"oj-doc-ti\", \"id\":None}).get_text() for annexe in annexes]\n",
    "\n",
    "text += [annexe.text.strip() for annexe in annexes]\n",
    "context += titres_annexes\n",
    "url += [ai_act_url+f\"#{annexe['id']}\" for annexe in annexes]\n",
    "url_sections += [None for _ in range(len(annexes))]\n",
    "url_chapitres += [ai_act_url+\"##anx_I\" for _ in range(len(annexes))]\n",
    "\n",
    "texts = [f\"{context[idx]} \\n {text[idx]}\" for idx in range(len(text))]\n",
    "metadatas = [ \n",
    "    {\n",
    "        \"titre\":context[idx],\n",
    "        \"url\":url[idx],\n",
    "        \"url_chapitre\": url_chapitres[idx],\n",
    "        \"url_section\": url_sections[idx]\n",
    "    }\n",
    "    for idx in range(len(texts))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ba4dc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'section_title': 'Ch.9-SURVEILLANCE APRÈS COMMERCIALISATION, PARTAGE D’INFORMATIONS ET SURVEILLANCE DU MARCHÉ > Sct.5-Surveillance, enquêtes, contrôle de l’application et contrôle en ce qui concerne les fournisseurs de modèles d’IA à\\xa0usage général > Art.91-\\nPouvoir de demander de la documentation et des informations\\n',\n",
       " 'url': 'https://eur-lex.europa.eu/legal-content/FR/TXT/HTML/?uri=OJ:L_202401689#art_91',\n",
       " 'url_chapitre': 'https://eur-lex.europa.eu/legal-content/FR/TXT/HTML/?uri=OJ:L_202401689#cpt_IX',\n",
       " 'url_section': 'https://eur-lex.europa.eu/legal-content/FR/TXT/HTML/?uri=OJ:L_202401689#cpt_IX.sct_5'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadatas[270]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165db10e",
   "metadata": {},
   "source": [
    "## Préparation des Documents et des chunks avec [RecursiveCharaterTextSplitter](https://python.langchain.com/docs/how_to/recursive_text_splitter/). \n",
    "\n",
    "&emsp;La séléction de la taille des chunks et leurs overlap peut avoir un impact significatif sur la performance de la recherche vectorielle par la suite et donc de la pérformance du RAG par derrieres. \n",
    "\n",
    "Ce [guide](https://www.machinelearningplus.com/gen-ai/optimizing-rag-chunk-size-your-definitive-guide-to-better-retrieval-accuracy/#:~:text=Optimal%20chunk%20size%20for%20RAG%20systems%20typically%20ranges,tokens%29%20provide%20better%20context%20for%20complex%20reasoning%20tasks.) donne une idée de structure d'évaluation pour détèrminer les bons paramétres. \n",
    "\n",
    "A titre d'example, on vas utiliser des chunks de 256 characters avec un overlap de 10%. Une petite taille de chunk fera de notre RAG un outil paré pour répondre à des questions factuelles.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47847c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 256, chunk_overlap = int(256*0.1), length_function=len\n",
    ")\n",
    "\n",
    "text_chunks: list[Document] = text_splitter.create_documents(texts=texts, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b2f24",
   "metadata": {},
   "source": [
    "## Création d'une base vectorielle. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9f8cca",
   "metadata": {},
   "source": [
    "### En local\n",
    "On peut créer et sauvgarder nos base vectoriel localement. FAISS est un exemple de base vectorielle que l'on peu créer avec LangChain et qui a l'avantage de nous éviter de créer un Index dans Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f385e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcf2de27f3e4ccc94df0af362ee6daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     10\u001b[39m embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n\u001b[32m     11\u001b[39m     azure_endpoint=os.environ[\u001b[33m\"\u001b[39m\u001b[33mAZURE_OPENAI_API_ENDPOINT\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     12\u001b[39m     azure_deployment=os.environ[\u001b[33m\"\u001b[39m\u001b[33mAZURE_OPENAI_EMBEDDING_MODEL\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     skip_empty=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     23\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Définition de l'index dans le quel nous allons stoquer notre base vectorielle.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m index = faiss.IndexFlatL2(\u001b[38;5;28mlen\u001b[39m(\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhello world\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[32m     28\u001b[39m vector_store = FAISS(\n\u001b[32m     29\u001b[39m     embedding_function=embeddings,\n\u001b[32m     30\u001b[39m     index=index,\n\u001b[32m     31\u001b[39m     docstore=InMemoryDocstore(),\n\u001b[32m     32\u001b[39m     index_to_docstore_id={},\n\u001b[32m     33\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HugoRECHATIN\\Documents\\Langchain RAG Run Through\\.venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:638\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    629\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[32m    630\u001b[39m \n\u001b[32m    631\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    636\u001b[39m \u001b[33;03m        Embedding for the text.\u001b[39;00m\n\u001b[32m    637\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HugoRECHATIN\\Documents\\Langchain RAG Run Through\\.venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:590\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    587\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    588\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m    589\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HugoRECHATIN\\Documents\\Langchain RAG Run Through\\.venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:478\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    482\u001b[39m         response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HugoRECHATIN\\Documents\\Langchain RAG Run Through\\.venv\\Lib\\site-packages\\openai\\resources\\embeddings.py:129\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    123\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    124\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    125\u001b[39m             ).tolist()\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HugoRECHATIN\\Documents\\Langchain RAG Run Through\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1237\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1245\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1246\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1247\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1248\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HugoRECHATIN\\Documents\\Langchain RAG Run Through\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# Définition du model utilisé pour véctoriser nos données. On utilise les clefs et endpoints du modéle de vectorisation déployé dans notre AI hub. \n",
    "embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_API_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    retry_max_seconds=60,\n",
    "    retry_min_seconds=10,\n",
    "    max_retries=5,\n",
    "    show_progress_bar=True,\n",
    "    chunk_size=512,\n",
    "    embedding_ctx_length=1000,\n",
    "    check_embedding_ctx_length=True,\n",
    "    skip_empty=True\n",
    ")\n",
    "\n",
    "# Définition de l'index dans le quel nous allons stoquer notre base vectorielle.\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a2941",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=os.environ[\"SEARCH_ENDPOINT\"],\n",
    "    azure_search_key=os.environ[\"SEARCH_KEY\"],\n",
    "    index_name=os.environ[\"SEARCH_INDEX\"],\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    additional_search_client_options={\"retry_total\": 4},\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
